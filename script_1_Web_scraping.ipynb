{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "script 1 Web scraping",
      "provenance": [],
      "authorship_tag": "ABX9TyPz2cQjs+1EyD8x7yXNYLX1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danjstr/M10_webscraper_assignment/blob/main/script_1_Web_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQQcYpKhs6Tg"
      },
      "source": [
        "####\n",
        "#Author: Daniel Strauss\n",
        "#version 1.0\n",
        "#references: \n",
        "#https://www.programiz.com/python-programming/working-csv-files\n",
        "#https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.create_bucket\n",
        "#https://realpython.com/python-boto3-aws-s3/\n",
        "#CLI aws s3api create-bucket --bucket my-bucket-name --region us-west-2 --create-bucket-configuration LocationConstraint=us-west-2\n",
        "#https://robertorocha.info/setting-up-a-selenium-web-scraper-on-aws-lambda-with-python/ \n",
        "##\n",
        "\n",
        "###Load libraries\n",
        "import awscli\n",
        "import selenium\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from selenium import webdriver\n",
        "\n",
        "\n",
        "####SCRAPE THE WEBSITE######\n",
        "###call the webdriver\n",
        "browser = webdriver.Chrome(\"C:\\Windows/chromedriver.exe\")\n",
        "\n",
        "#Path that needs to be accessed by webdriver\n",
        "browser.get('https://www.charitiesnys.com/RegistrySearch/search_charities.jsp')\n",
        "\n",
        "#identify xpath of location to select element\n",
        "inputElement = browser.find_element_by_xpath(\"/html/body/div/div[2]/div/table/tbody/tr/td[2]/div/div/font/font/font/font/font/font/table/tbody/tr[4]/td/form/table/tbody/tr[2]/td[2]/input[1]\")\n",
        "inputElement.send_keys('0')\n",
        "inputElement1 = browser.find_element_by_xpath(\"/html/body/div/div[2]/div/table/tbody/tr/td[2]/div/div/font/font/font/font/font/font/table/tbody/tr[4]/td/form/table/tbody/tr[10]/td/input[1]\").click()\n",
        "\n",
        "#identify the table to scrape\n",
        "table = browser.find_element_by_css_selector('table.Bordered')\n",
        "\n",
        "print(table)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sypSZHKWrblY"
      },
      "source": [
        "\n",
        "#####CREATE DATE FRAME#####\n",
        "#create empty dataframe\n",
        "df =[]\n",
        "\n",
        "#loop through dataframe to export table\n",
        "for row in table.find_elements_by_css_selector('tr'):\n",
        "      cols = df.append([cell.text for cell in row.find_elements_by_css_selector('td')])\n",
        "\n",
        "\n",
        "#update dataframe with header \n",
        "df = pd.DataFrame(df, columns = [\"Organization Name\", \"NY Reg #\", \"EIN\" ,\"Registrant Type\",\"City\",\"State\"])\n",
        "# display(df) #let's have a look at the data before creating the CSV file and loading it into s3\n",
        "\n",
        "# Removing row with \"None\" values and re-indexing the dataframe\n",
        "updated_df = pd.DataFrame(df).dropna().reset_index().drop(['index'], axis=1) \n",
        "\n",
        "updated_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5NABYoTs_hP"
      },
      "source": [
        "\n",
        "###LOAD THE FILE INTO S3####\n",
        "# prepare csv file name   \n",
        "pathname = 's3:/yubucket1/'\n",
        "filename= 'charities_bureau_scrape_' #name of your group\n",
        "datetime = time.strftime(\"%Y%m%d%H%M%S\") #timestamp\n",
        "filenames3 = \"%s%s%s.csv\"%(pathname,filename,datetime) #name of the filepath and csv file\n",
        "\n",
        "#load file into s3. Pandas actually leverages boto to connect to s3 and can push the file directly into an s3 bucket\n",
        "updated_df.to_csv(filenames3, header=True, line_terminator='\\n') \n",
        "\n",
        "#print success message\n",
        "print(\"Successfull uploaded file to location:\"+str(filenames3))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}